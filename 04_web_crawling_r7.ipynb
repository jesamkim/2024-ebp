{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf5d11ed",
   "metadata": {},
   "source": [
    "# 0. 패키지 인스톨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fd765c5-424b-4577-a5f2-547c9a3d1a6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (1.34.89)\n",
      "Requirement already satisfied: botocore in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (1.34.89)\n",
      "Requirement already satisfied: anthropic in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (0.25.6)\n",
      "Requirement already satisfied: langchain==0.1.16 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.1.16)\n",
      "Requirement already satisfied: langchain_aws==0.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.1.2)\n",
      "Requirement already satisfied: langchain_community==0.0.34 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (0.0.34)\n",
      "Requirement already satisfied: opensearch-py==2.4.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (2.4.2)\n",
      "Requirement already satisfied: pdfplumber==0.10.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (0.10.3)\n",
      "Requirement already satisfied: termcolor==2.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (2.4.0)\n",
      "Requirement already satisfied: opensearch_dsl==2.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (2.1.0)\n",
      "Requirement already satisfied: pdf2image in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (1.17.0)\n",
      "Requirement already satisfied: pikepdf==8.12.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (8.12.0)\n",
      "Requirement already satisfied: pypdf==4.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (4.0.1)\n",
      "Collecting selenium (from -r requirements.txt (line 14))\n",
      "  Using cached selenium-4.20.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting trafilatura (from -r requirements.txt (line 15))\n",
      "  Using cached trafilatura-1.8.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: openpyxl in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 16)) (3.1.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain==0.1.16->-r requirements.txt (line 4)) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain==0.1.16->-r requirements.txt (line 4)) (2.0.29)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain==0.1.16->-r requirements.txt (line 4)) (3.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain==0.1.16->-r requirements.txt (line 4)) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain==0.1.16->-r requirements.txt (line 4)) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain==0.1.16->-r requirements.txt (line 4)) (1.33)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.42 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain==0.1.16->-r requirements.txt (line 4)) (0.1.46)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain==0.1.16->-r requirements.txt (line 4)) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain==0.1.16->-r requirements.txt (line 4)) (0.1.51)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain==0.1.16->-r requirements.txt (line 4)) (1.22.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain==0.1.16->-r requirements.txt (line 4)) (2.7.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain==0.1.16->-r requirements.txt (line 4)) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain==0.1.16->-r requirements.txt (line 4)) (8.2.3)\n",
      "Requirement already satisfied: urllib3>=1.26.18 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opensearch-py==2.4.2->-r requirements.txt (line 7)) (2.2.1)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opensearch-py==2.4.2->-r requirements.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opensearch-py==2.4.2->-r requirements.txt (line 7)) (2.9.0)\n",
      "Requirement already satisfied: certifi>=2022.12.07 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opensearch-py==2.4.2->-r requirements.txt (line 7)) (2024.2.2)\n",
      "Requirement already satisfied: pdfminer.six==20221105 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pdfplumber==0.10.3->-r requirements.txt (line 8)) (20221105)\n",
      "Requirement already satisfied: Pillow>=9.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pdfplumber==0.10.3->-r requirements.txt (line 8)) (10.2.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pdfplumber==0.10.3->-r requirements.txt (line 8)) (4.29.0)\n",
      "Requirement already satisfied: Deprecated in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pikepdf==8.12.0->-r requirements.txt (line 12)) (1.2.14)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pikepdf==8.12.0->-r requirements.txt (line 12)) (23.2)\n",
      "Requirement already satisfied: lxml>=4.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pikepdf==8.12.0->-r requirements.txt (line 12)) (5.2.1)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pdfminer.six==20221105->pdfplumber==0.10.3->-r requirements.txt (line 8)) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pdfminer.six==20221105->pdfplumber==0.10.3->-r requirements.txt (line 8)) (42.0.5)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3->-r requirements.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3->-r requirements.txt (line 1)) (0.10.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anthropic->-r requirements.txt (line 3)) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anthropic->-r requirements.txt (line 3)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anthropic->-r requirements.txt (line 3)) (0.27.0)\n",
      "Requirement already satisfied: sniffio in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anthropic->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anthropic->-r requirements.txt (line 3)) (0.19.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anthropic->-r requirements.txt (line 3)) (4.10.0)\n",
      "Collecting trio~=0.17 (from selenium->-r requirements.txt (line 14))\n",
      "  Using cached trio-0.25.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium->-r requirements.txt (line 14))\n",
      "  Using cached trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting courlan>=1.0.0 (from trafilatura->-r requirements.txt (line 15))\n",
      "  Downloading courlan-1.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting htmldate>=1.8.0 (from trafilatura->-r requirements.txt (line 15))\n",
      "  Using cached htmldate-1.8.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting justext>=3.0.0 (from trafilatura->-r requirements.txt (line 15))\n",
      "  Using cached jusText-3.0.0-py2.py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting lxml>=4.8 (from pikepdf==8.12.0->-r requirements.txt (line 12))\n",
      "  Using cached lxml-5.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: et-xmlfile in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from openpyxl->-r requirements.txt (line 16)) (1.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16->-r requirements.txt (line 4)) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16->-r requirements.txt (line 4)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16->-r requirements.txt (line 4)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16->-r requirements.txt (line 4)) (1.9.4)\n",
      "Requirement already satisfied: idna>=2.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->anthropic->-r requirements.txt (line 3)) (3.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->anthropic->-r requirements.txt (line 3)) (1.2.0)\n",
      "Requirement already satisfied: babel>=2.11.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from courlan>=1.0.0->trafilatura->-r requirements.txt (line 15)) (2.14.0)\n",
      "Collecting tld>=0.13 (from courlan>=1.0.0->trafilatura->-r requirements.txt (line 15))\n",
      "  Using cached tld-0.13-py2.py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.16->-r requirements.txt (line 4)) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.16->-r requirements.txt (line 4)) (0.9.0)\n",
      "Collecting dateparser>=1.1.2 (from htmldate>=1.8.0->trafilatura->-r requirements.txt (line 15))\n",
      "  Using cached dateparser-1.2.0-py2.py3-none-any.whl.metadata (28 kB)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->anthropic->-r requirements.txt (line 3)) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic->-r requirements.txt (line 3)) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.16->-r requirements.txt (line 4)) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.16->-r requirements.txt (line 4)) (3.10.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.1.16->-r requirements.txt (line 4)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.1.16->-r requirements.txt (line 4)) (2.18.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.16->-r requirements.txt (line 4)) (3.0.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from tokenizers>=0.13.0->anthropic->-r requirements.txt (line 3)) (0.22.2)\n",
      "Requirement already satisfied: sortedcontainers in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from trio~=0.17->selenium->-r requirements.txt (line 14)) (2.4.0)\n",
      "Collecting outcome (from trio~=0.17->selenium->-r requirements.txt (line 14))\n",
      "  Using cached outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium->-r requirements.txt (line 14))\n",
      "  Using cached wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from urllib3[socks]<3,>=1.26->selenium->-r requirements.txt (line 14)) (1.7.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from Deprecated->pikepdf==8.12.0->-r requirements.txt (line 12)) (1.16.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber==0.10.3->-r requirements.txt (line 8)) (1.16.0)\n",
      "Requirement already satisfied: pytz in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from dateparser>=1.1.2->htmldate>=1.8.0->trafilatura->-r requirements.txt (line 15)) (2024.1)\n",
      "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from dateparser>=1.1.2->htmldate>=1.8.0->trafilatura->-r requirements.txt (line 15)) (2023.12.25)\n",
      "Collecting tzlocal (from dateparser>=1.1.2->htmldate>=1.8.0->trafilatura->-r requirements.txt (line 15))\n",
      "  Using cached tzlocal-5.2-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic->-r requirements.txt (line 3)) (3.13.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic->-r requirements.txt (line 3)) (2024.3.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic->-r requirements.txt (line 3)) (4.66.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.16->-r requirements.txt (line 4)) (1.0.0)\n",
      "Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber==0.10.3->-r requirements.txt (line 8)) (2.21)\n",
      "Using cached selenium-4.20.0-py3-none-any.whl (9.5 MB)\n",
      "Using cached trafilatura-1.8.1-py3-none-any.whl (1.0 MB)\n",
      "Downloading courlan-1.1.0-py3-none-any.whl (33 kB)\n",
      "Using cached htmldate-1.8.1-py3-none-any.whl (31 kB)\n",
      "Using cached jusText-3.0.0-py2.py3-none-any.whl (837 kB)\n",
      "Using cached lxml-5.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "Using cached trio-0.25.0-py3-none-any.whl (467 kB)\n",
      "Using cached trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Using cached dateparser-1.2.0-py2.py3-none-any.whl (294 kB)\n",
      "Using cached tld-0.13-py2.py3-none-any.whl (263 kB)\n",
      "Using cached wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Using cached outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Using cached tzlocal-5.2-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: wsproto, tzlocal, tld, outcome, lxml, trio, justext, dateparser, courlan, trio-websocket, htmldate, trafilatura, selenium\n",
      "  Attempting uninstall: lxml\n",
      "    Found existing installation: lxml 5.2.1\n",
      "    Uninstalling lxml-5.2.1:\n",
      "      Successfully uninstalled lxml-5.2.1\n",
      "Successfully installed courlan-1.1.0 dateparser-1.2.0 htmldate-1.8.1 justext-3.0.0 lxml-5.1.1 outcome-1.3.0.post0 selenium-4.20.0 tld-0.13 trafilatura-1.8.1 trio-0.25.0 trio-websocket-0.11.1 tzlocal-5.2 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd754137-3f08-421d-bbf1-47377864fb12",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Selenium을 활용한 크롤링 샘플 코드\n",
    "> Chrome 및 Chrom driver의 stable 버전은 [이 URL](https://googlechromelabs.github.io/chrome-for-testing/#stable) 에서 다운로드 받았습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13313468-a4be-4c62-8a4f-25c4829809b5",
   "metadata": {},
   "source": [
    "#### 2-1. 리눅스용 Chrome Driver (123.0.6312.105 버전) 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b62c61d9-84e3-4aba-889d-85759f254850",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-30 13:23:58--  https://storage.googleapis.com/chrome-for-testing-public/123.0.6312.105/linux64/chromedriver-linux64.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.14.219, 172.217.14.251, 142.250.69.219, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.14.219|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8636471 (8.2M) [application/zip]\n",
      "Saving to: ‘chromedriver-linux64.zip’\n",
      "\n",
      "100%[======================================>] 8,636,471   --.-K/s   in 0.07s   \n",
      "\n",
      "2024-04-30 13:23:58 (117 MB/s) - ‘chromedriver-linux64.zip’ saved [8636471/8636471]\n",
      "\n",
      "Archive:  chromedriver-linux64.zip\n",
      "  inflating: chromedriver-linux64/LICENSE.chromedriver  \n",
      "  inflating: chromedriver-linux64/chromedriver  \n"
     ]
    }
   ],
   "source": [
    "## Chrome 드라이버 다운로드 및 설치는 필수 입니다.\n",
    "\n",
    "!wget https://storage.googleapis.com/chrome-for-testing-public/123.0.6312.105/linux64/chromedriver-linux64.zip\n",
    "\n",
    "!unzip -o chromedriver-linux64.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5160b19f-ae0f-4229-b64f-9ca31818a30b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromeDriver 123.0.6312.105 (399174dbe6eff0f59de9a6096129c0c827002b3a-refs/branch-heads/6312@{#761})\n"
     ]
    }
   ],
   "source": [
    "# Chrome 드라이버 버전 확인\n",
    "\n",
    "!./chromedriver-linux64/chromedriver --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f920194-169a-4696-bd03-fa619d166582",
   "metadata": {},
   "source": [
    "#### 2-2. Google-chome (123.0.6312.105 버전) 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afcf3bae-5f61-4e7a-bf33-f0abe0f1e007",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-30 13:24:03--  https://storage.googleapis.com/chrome-for-testing-public/123.0.6312.105/linux64/chrome-linux64.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.215.251, 142.250.217.123, 142.250.217.91, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.215.251|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 150006507 (143M) [application/zip]\n",
      "Saving to: ‘chrome-linux64.zip’\n",
      "\n",
      "100%[======================================>] 150,006,507 76.7MB/s   in 1.9s   \n",
      "\n",
      "2024-04-30 13:24:05 (76.7 MB/s) - ‘chrome-linux64.zip’ saved [150006507/150006507]\n",
      "\n",
      "Archive:  chrome-linux64.zip\n",
      "  inflating: chrome-linux64/ABOUT    \n",
      "  inflating: chrome-linux64/MEIPreload/manifest.json  \n",
      "  inflating: chrome-linux64/MEIPreload/preloaded_data.pb  \n",
      "  inflating: chrome-linux64/chrome   \n",
      "  inflating: chrome-linux64/chrome-wrapper  \n",
      "  inflating: chrome-linux64/chrome_100_percent.pak  \n",
      "  inflating: chrome-linux64/chrome_200_percent.pak  \n",
      "  inflating: chrome-linux64/chrome_crashpad_handler  \n",
      "  inflating: chrome-linux64/chrome_sandbox  \n",
      "  inflating: chrome-linux64/icudtl.dat  \n",
      "  inflating: chrome-linux64/libEGL.so  \n",
      "  inflating: chrome-linux64/libGLESv2.so  \n",
      "  inflating: chrome-linux64/libvk_swiftshader.so  \n",
      "  inflating: chrome-linux64/libvulkan.so.1  \n",
      " extracting: chrome-linux64/product_logo_48.png  \n",
      "  inflating: chrome-linux64/resources.pak  \n",
      "  inflating: chrome-linux64/v8_context_snapshot.bin  \n",
      "  inflating: chrome-linux64/vk_swiftshader_icd.json  \n",
      "  inflating: chrome-linux64/xdg-mime  \n",
      "  inflating: chrome-linux64/xdg-settings  \n",
      "  inflating: chrome-linux64/locales/af.pak.info  \n",
      "  inflating: chrome-linux64/locales/ja.pak  \n",
      "  inflating: chrome-linux64/locales/fil.pak  \n",
      "  inflating: chrome-linux64/locales/ca.pak.info  \n",
      "  inflating: chrome-linux64/locales/es.pak.info  \n",
      "  inflating: chrome-linux64/locales/th.pak  \n",
      "  inflating: chrome-linux64/locales/en-GB.pak.info  \n",
      "  inflating: chrome-linux64/locales/ro.pak  \n",
      "  inflating: chrome-linux64/locales/fil.pak.info  \n",
      "  inflating: chrome-linux64/locales/en-US.pak.info  \n",
      "  inflating: chrome-linux64/locales/hi.pak.info  \n",
      "  inflating: chrome-linux64/locales/sw.pak.info  \n",
      "  inflating: chrome-linux64/locales/lv.pak.info  \n",
      "  inflating: chrome-linux64/locales/sw.pak  \n",
      "  inflating: chrome-linux64/locales/uk.pak  \n",
      "  inflating: chrome-linux64/locales/de.pak.info  \n",
      "  inflating: chrome-linux64/locales/sv.pak.info  \n",
      "  inflating: chrome-linux64/locales/et.pak.info  \n",
      "  inflating: chrome-linux64/locales/ko.pak  \n",
      "  inflating: chrome-linux64/locales/zh-CN.pak  \n",
      "  inflating: chrome-linux64/locales/bg.pak  \n",
      "  inflating: chrome-linux64/locales/sl.pak.info  \n",
      "  inflating: chrome-linux64/locales/id.pak.info  \n",
      "  inflating: chrome-linux64/locales/pl.pak.info  \n",
      "  inflating: chrome-linux64/locales/ru.pak.info  \n",
      "  inflating: chrome-linux64/locales/it.pak.info  \n",
      "  inflating: chrome-linux64/locales/ko.pak.info  \n",
      "  inflating: chrome-linux64/locales/mr.pak.info  \n",
      "  inflating: chrome-linux64/locales/mr.pak  \n",
      "  inflating: chrome-linux64/locales/es.pak  \n",
      "  inflating: chrome-linux64/locales/cs.pak.info  \n",
      "  inflating: chrome-linux64/locales/hr.pak.info  \n",
      "  inflating: chrome-linux64/locales/cs.pak  \n",
      "  inflating: chrome-linux64/locales/lt.pak  \n",
      "  inflating: chrome-linux64/locales/ml.pak.info  \n",
      "  inflating: chrome-linux64/locales/te.pak  \n",
      "  inflating: chrome-linux64/locales/ta.pak.info  \n",
      "  inflating: chrome-linux64/locales/ca.pak  \n",
      "  inflating: chrome-linux64/locales/gu.pak  \n",
      "  inflating: chrome-linux64/locales/ar.pak  \n",
      "  inflating: chrome-linux64/locales/ms.pak  \n",
      "  inflating: chrome-linux64/locales/uk.pak.info  \n",
      "  inflating: chrome-linux64/locales/da.pak  \n",
      "  inflating: chrome-linux64/locales/nb.pak.info  \n",
      "  inflating: chrome-linux64/locales/am.pak.info  \n",
      "  inflating: chrome-linux64/locales/ml.pak  \n",
      "  inflating: chrome-linux64/locales/ro.pak.info  \n",
      "  inflating: chrome-linux64/locales/ur.pak.info  \n",
      "  inflating: chrome-linux64/locales/sk.pak  \n",
      "  inflating: chrome-linux64/locales/es-419.pak  \n",
      "  inflating: chrome-linux64/locales/th.pak.info  \n",
      "  inflating: chrome-linux64/locales/en-GB.pak  \n",
      "  inflating: chrome-linux64/locales/he.pak  \n",
      "  inflating: chrome-linux64/locales/ta.pak  \n",
      "  inflating: chrome-linux64/locales/hu.pak.info  \n",
      "  inflating: chrome-linux64/locales/lt.pak.info  \n",
      "  inflating: chrome-linux64/locales/te.pak.info  \n",
      "  inflating: chrome-linux64/locales/et.pak  \n",
      "  inflating: chrome-linux64/locales/zh-TW.pak.info  \n",
      "  inflating: chrome-linux64/locales/af.pak  \n",
      "  inflating: chrome-linux64/locales/zh-TW.pak  \n",
      "  inflating: chrome-linux64/locales/zh-CN.pak.info  \n",
      "  inflating: chrome-linux64/locales/id.pak  \n",
      "  inflating: chrome-linux64/locales/hi.pak  \n",
      "  inflating: chrome-linux64/locales/ja.pak.info  \n",
      "  inflating: chrome-linux64/locales/kn.pak  \n",
      "  inflating: chrome-linux64/locales/bn.pak  \n",
      "  inflating: chrome-linux64/locales/el.pak  \n",
      "  inflating: chrome-linux64/locales/en-US.pak  \n",
      "  inflating: chrome-linux64/locales/el.pak.info  \n",
      "  inflating: chrome-linux64/locales/sr.pak  \n",
      "  inflating: chrome-linux64/locales/fr.pak  \n",
      "  inflating: chrome-linux64/locales/ar.pak.info  \n",
      "  inflating: chrome-linux64/locales/de.pak  \n",
      "  inflating: chrome-linux64/locales/vi.pak  \n",
      "  inflating: chrome-linux64/locales/pt-BR.pak  \n",
      "  inflating: chrome-linux64/locales/hu.pak  \n",
      "  inflating: chrome-linux64/locales/nl.pak.info  \n",
      "  inflating: chrome-linux64/locales/bg.pak.info  \n",
      "  inflating: chrome-linux64/locales/gu.pak.info  \n",
      "  inflating: chrome-linux64/locales/bn.pak.info  \n",
      "  inflating: chrome-linux64/locales/pt-PT.pak.info  \n",
      "  inflating: chrome-linux64/locales/kn.pak.info  \n",
      "  inflating: chrome-linux64/locales/hr.pak  \n",
      "  inflating: chrome-linux64/locales/fi.pak.info  \n",
      "  inflating: chrome-linux64/locales/pt-PT.pak  \n",
      "  inflating: chrome-linux64/locales/ms.pak.info  \n",
      "  inflating: chrome-linux64/locales/ur.pak  \n",
      "  inflating: chrome-linux64/locales/it.pak  \n",
      "  inflating: chrome-linux64/locales/fr.pak.info  \n",
      "  inflating: chrome-linux64/locales/pl.pak  \n",
      "  inflating: chrome-linux64/locales/es-419.pak.info  \n",
      "  inflating: chrome-linux64/locales/sr.pak.info  \n",
      "  inflating: chrome-linux64/locales/fa.pak.info  \n",
      "  inflating: chrome-linux64/locales/tr.pak  \n",
      "  inflating: chrome-linux64/locales/pt-BR.pak.info  \n",
      "  inflating: chrome-linux64/locales/fa.pak  \n",
      "  inflating: chrome-linux64/locales/vi.pak.info  \n",
      "  inflating: chrome-linux64/locales/he.pak.info  \n",
      "  inflating: chrome-linux64/locales/nb.pak  \n",
      "  inflating: chrome-linux64/locales/lv.pak  \n",
      "  inflating: chrome-linux64/locales/nl.pak  \n",
      "  inflating: chrome-linux64/locales/fi.pak  \n",
      "  inflating: chrome-linux64/locales/ru.pak  \n",
      "  inflating: chrome-linux64/locales/da.pak.info  \n",
      "  inflating: chrome-linux64/locales/tr.pak.info  \n",
      "  inflating: chrome-linux64/locales/sk.pak.info  \n",
      "  inflating: chrome-linux64/locales/sl.pak  \n",
      "  inflating: chrome-linux64/locales/am.pak  \n",
      "  inflating: chrome-linux64/locales/sv.pak  \n",
      "  inflating: chrome-linux64/resources/inspector_overlay/main.js  \n",
      "  inflating: chrome-linux64/resources/inspector_overlay/inspector_overlay_resources.grd  \n"
     ]
    }
   ],
   "source": [
    "## Chrome 다운로드 및 설치는 필수 입니다.\n",
    "\n",
    "!wget https://storage.googleapis.com/chrome-for-testing-public/123.0.6312.105/linux64/chrome-linux64.zip\n",
    "\n",
    "!unzip -o chrome-linux64.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8646e80d-b91e-4255-96ce-c753c31b33e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Chrome for Testing 123.0.6312.105 \n"
     ]
    }
   ],
   "source": [
    "# Chrome 버전 확인\n",
    "\n",
    "!./chrome-linux64/chrome --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cc4a937-2f98-49aa-a928-f92e08a542f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (4.20.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.1)\n",
      "Requirement already satisfied: trio~=0.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from selenium) (0.25.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from selenium) (2024.2.2)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from selenium) (4.10.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from trio~=0.17->selenium) (3.6)\n",
      "Requirement already satisfied: outcome in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26a39ba-9bf3-4ed0-9daa-25ee43d03898",
   "metadata": {},
   "source": [
    "#### 2-3. Chrome Driver 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b0f676a-23dd-473f-a3c4-3a5dbade06fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Selenium 드라이버 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "chrome_options.binary_location = './chrome-linux64/chrome'  # 크롬 브라우저 경로 지정\n",
    "\n",
    "# 크롬 드라이버 경로 설정\n",
    "driver_path = \"./chromedriver-linux64/chromedriver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53f4eb84-e57a-462d-a545-f18b049f83cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 크롬 드라이버 서비스 객체 생성\n",
    "service = Service(driver_path)\n",
    "\n",
    "# 크롬 드라이버 실행\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec56ba3-ebcd-4086-b2cd-22671d0075fd",
   "metadata": {},
   "source": [
    "#### 2-3. 크롤링 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e4deafc-3270-4427-b541-d427381bc020",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "# 오늘 날짜 가져오기 (YYYY-MM-DD)\n",
    "today = date.today().strftime('%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "351ac2d4-e572-4730-91d3-35fe9f9fad42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/2024-ebp/crawling/2024-04-30 디렉토리가 생성되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 크롤링 결과 저장 디렉토리 지정 (ex: ./crawling/2024-04-30/)\n",
    "\n",
    "import os\n",
    "\n",
    "# 현재 작업 디렉토리 경로 가져오기\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# 생성할 디렉토리 경로\n",
    "temp_dir = os.path.join(current_dir, f\"crawling/{today}\")\n",
    "\n",
    "# 디렉토리가 없으면 생성\n",
    "if not os.path.exists(temp_dir):\n",
    "    os.makedirs(temp_dir)\n",
    "    print(f\"{temp_dir} 디렉토리가 생성되었습니다.\")\n",
    "else:\n",
    "    print(f\"{temp_dir} 디렉토리가 이미 존재합니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a89090f-fef8-482c-87bc-e13f5e5b6140",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********1**********\n",
      "**********2**********\n",
      "**********3**********\n",
      "**********4**********\n",
      "**********5**********\n",
      "**********6**********\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import trafilatura   #본문만 갖고 오는 함수\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "words = ['BESS project'] # 'solar PV project' #,'AUSTRALIA SOLAR FARM','AUSTRALIA BESS','SAUDI PV','SAUDI SOLAR FARM','SAUDI BESS','nuclear power plant']\n",
    "\n",
    "word_list = []\n",
    "title_list = []\n",
    "content_list = []\n",
    "link_list = []\n",
    "date_list = []\n",
    "\n",
    "url = 'https://www.google.com/search?q={}&newwindow=1&sca_esv=600662400&tbs=cdr:1,cd_min:1/1/2024,cd_max:4/16/2024&tbm=nws&ei=vGmvZaObBLPd1e8PirSVkAY&start={}&sa=N&ved=2ahUKEwij6Ye5_fKDAxWzbvUHHQpaBWI4KBDy0wN6BAgEEAQ&biw=1536&bih=735&dpr=1.25'\n",
    "#날짜 설정\n",
    "\n",
    "for word in words:\n",
    "    page = 1\n",
    "    for i in range(0, 60, 10):     #페이지 수   \n",
    "        # url {}에 순서대로 word,i 넣기\n",
    "        new_url = url.format(word, i)\n",
    "\n",
    "        # 크롬 드라이버에 url 주소 넣고 실행\n",
    "        driver.get(new_url)\n",
    "\n",
    "        # 페이지가 완전히 로딩되도록 3초동안 기다림\n",
    "        time.sleep(3)\n",
    "\n",
    "        print(\"*\" * 10 + str(page) + \"*\" * 10)\n",
    "        page = page + 1\n",
    "        \n",
    "        \n",
    "        titles = driver.find_elements(By.CLASS_NAME, 'n0jPhd.ynAwRc')\n",
    "        for title in titles:\n",
    "            word_list.append(word)\n",
    "            title_list.append(title.text.replace(\",\" , \"\"))\n",
    "             # print(title.text) # 기사 제목 확인\n",
    "\n",
    "        contents = driver.find_elements(By.CLASS_NAME, 'GI74Re.nDgy9d')\n",
    "        for content in contents:\n",
    "            content_list.append(content.text.replace(\",\" , \"\"))\n",
    "\n",
    "        dates = driver.find_elements(By.CLASS_NAME, 'OSrXXb.rbYSKb.LfVVr')\n",
    "        for date in dates:\n",
    "            date_list.append(date.text.replace(\",\" , \"\"))\n",
    "\n",
    "        links = driver.find_elements(By.CLASS_NAME, 'WlydOe')\n",
    "        for link in links:\n",
    "            link_list.append(link.get_attribute('href'))\n",
    "\n",
    "        if page == 32:  # 페이지 수정\n",
    "            break\n",
    "\n",
    "\n",
    "# 링크내 기사 본문 갖고 오기\n",
    "\n",
    "def extract_content(link_list):  \n",
    "    content_full = []\n",
    "    for link in link_list:\n",
    "        try:\n",
    "            downloaded = trafilatura.fetch_url(link)\n",
    "            extracted = trafilatura.extract(downloaded)\n",
    "            content_full.append(extracted)\n",
    "        except:\n",
    "            content_full.append(None)\n",
    "    return content_full\n",
    "\n",
    "# 사용 예시\n",
    "# link_list = ['https://www.example.com/article1', 'https://www.example.com/article2', 'https://www.example.com/article3']\n",
    "content_full = extract_content(link_list)\n",
    "\n",
    "# 결과 확인\n",
    "len(content_full)            \n",
    "            \n",
    "data = {'WORD': word_list, 'TITLE': title_list, 'CONTENT': content_list, 'CONTENT_FULL': content_full,'LINK': link_list, 'DATE': date_list}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(f\"{temp_dir}/{today}_origin.csv\", encoding=\"utf-8-sig\")       # 일별 파일명설정\n",
    "df.to_excel(f\"{temp_dir}/{today}_origin.xlsx\")\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39001b2b-0aca-4931-b3cb-5a7d9e363ff7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " len(link_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad21c28-7403-4cfe-a229-b8be3df2a298",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. LangChain을 활용한 Bedrock Claude 3 호출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c178c1-add8-44be-ae1c-5c9113b63db8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2-1. LangChain의 ChatBedrock을 활용한 가장 간단한 코드 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b4a7e40-a522-44e4-8879-66b843408261",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# len(link_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac8833d-d23f-4208-ac4f-724fd6e4b224",
   "metadata": {},
   "source": [
    "## 3. LLM 활용 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5563b5-8330-48b7-aa7e-e4876a51be60",
   "metadata": {},
   "source": [
    "### 3.1 기사내용 요약, 국가, 도시 찾기 후 파싱하여 list로 넣기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75c0752e-69dd-4c5d-b5f5-5c782114d2cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['California', 'Texas', 'Arizona', 'Texas, Arizona', 'Utah', 'Arizona', 'Arizona', 'Michigan', 'Indiana', 'null']\n",
      "['Arizona', 'Kern County', 'Fatehgarh, Uttar Pradesh', 'Uusikaupunki', 'Grand Terrace', 'null', 'Menifee', 'Nurmijärvi', 'Qianjiang', 'Kern County']\n",
      "['Hawaii', 'South Australia', 'null', 'New York City', 'Vlissingen', 'Austin', 'Groningen', 'null', 'Galveston County', 'null']\n",
      "['Arizona', 'Texas', 'Rajnandgaon', 'Antofagasta', 'null', 'null', 'Flevoland', 'Lappeenranta', 'South Australia', 'Texas']\n",
      "['Illinois', 'Antofagasta', 'Melbourne', 'New Mexico', 'Arizona', 'Morro Bay', 'Warrington', 'California', 'null', 'Västernorrland']\n",
      "['Noordoostpolder', 'null', 'Kilmarnock', 'null', 'Lolland', 'Antofagasta', 'Vilvoorde', 'Arizona', 'California', 'null']\n",
      "['null']\n",
      "[]\n",
      "[]\n",
      "['null']\n",
      "[]\n",
      "62\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#from langchain_community.chat_models import BedrockChat\n",
    "from langchain_aws import ChatBedrock\n",
    "from botocore.config import Config\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "config = Config(read_timeout=1000)\n",
    "\n",
    "def parsing_data(content):\n",
    "    summary_list = []\n",
    "    summary_kor_list = []\n",
    "    country_list =[]\n",
    "    city_list =[]\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    \n",
    "    for i in range(0, len(content_full)):\n",
    "        summary = soup.find(f'summary_{i}')\n",
    "        # summary_kor = soup.find(f'summary_kor_{i}')\n",
    "        city = soup.find(f'city_{i}')\n",
    "        country = soup.find(f'country_{i}')\n",
    "        if summary:\n",
    "            summary_list.append(summary.text.strip())\n",
    "\n",
    "            country_list.append(country.text.strip() if country is not None else 'NaN')\n",
    "            city_list.append(city.text.strip() if city is not None else 'NaN')\n",
    "\n",
    "            # summary_kor_list.append(summary.text.s trip())        \n",
    "            # country_list.append(country.text.strip())\n",
    "            # city_list.append(city.text.strip())\n",
    "    # print(summary_list)\n",
    "    print(city_list)\n",
    "    return summary_list, country_list, city_list\n",
    "\n",
    "def get_text_response(input_content):\n",
    "    llm = ChatBedrock(\n",
    "        credentials_profile_name=os.environ.get(\"BWB_PROFILE_NAME\"),\n",
    "        region_name=os.environ.get(\"BWB_REGION_NAME\"),\n",
    "        endpoint_url=os.environ.get(\"BWB_ENDPOINT_URL\"),\n",
    "        model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "        model_kwargs={\n",
    "            \"max_tokens\": 4096,   # maximum : 4096\n",
    "            \"temperature\": 0,\n",
    "            \"top_p\": 0.01,\n",
    "            \"top_k\": 0,\n",
    "        } , \n",
    "        config = config\n",
    "    )\n",
    "    return llm.predict(input_content)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    response_content = []\n",
    "    total_summary_list =[]\n",
    "    total_country_list = []\n",
    "    total_city_list = [] \n",
    "\n",
    "    for start in range(0, 101, 10):\n",
    "        end = start + 10\n",
    "        content_prompt = \"\"\n",
    "        for doc_i in range(start, end):\n",
    "            if doc_i < len(content_full):\n",
    "                content_prompt += f\"\"\"<content_{doc_i}>{content_full[doc_i]}</content_{doc_i}>\"\"\"\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "\n",
    "        다음 <content></content> 태그 안에 있는 본문의 내용을 <instructions></instructions> 지침에 따라 자세한 요약 메모를 작성하세요:\n",
    "\n",
    "        <content> \n",
    "        {content_prompt}\n",
    "        </content> \n",
    "\n",
    "        <instructions>\n",
    "          각 <content> 기사의 주요 내용을 3~5문장으로 영어로 요약하고 요약 내용을 list에 삽입해줘.  \n",
    "          요약시 상세한 숫자를 기입해서 작성해줘.\n",
    "          각 <content> 에서 해당 국가정보와 도시정보가 있으면 국가정보는 country에, 도시정보는 city에 각각 영어로 한개의 값만 list에 삽입해주고 해당 정보가 없으면 'null'으로 처리해줘.\n",
    "\n",
    "          summary list 는 <summary_NUMBER> 태그 안에 넣어서 출력해줘\n",
    "          country list 는 <country_NUMBER> 태그 안에 넣어서 출력해줘\n",
    "          city list 는 <city_NUMBER> 태그 안에 넣어서 출력해줘\n",
    "        </instructions>\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        response = get_text_response(input_content=prompt)\n",
    "        response_content.append(response)\n",
    "        \n",
    "    for i, content in enumerate(response_content):\n",
    "        # print(f\"Response {i}:\")\n",
    "        # print(content)\n",
    "        summary_list, country_list, city_list = parsing_data(content)\n",
    "        total_summary_list.extend(summary_list)\n",
    "        total_country_list.extend(country_list)\n",
    "        total_city_list.extend(city_list)\n",
    "\n",
    "print(len(total_summary_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4862a1b9-4b2b-435b-b547-c7d0440c9b5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(link_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caab46c6-9a12-4a69-9f1f-e6cdc5366fae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae3d2749-56fb-4b10-817b-532a1f8e26e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.2 LLM으로 날짜 수정하기 (잘안됨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8ccb75-2c13-4059-8857-1e1786d7cba6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f03a8e5-0623-47d0-8946-f483ccc592ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5eb6ed49-8222-4f2b-9d34-1407b87c5707",
   "metadata": {},
   "source": [
    "### 기존 결과값이랑 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23e8c92c-f1d9-4f05-b9f9-7f1bfbb1520d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # CITY 열의 공백 값을 np.nan으로 대체\n",
    "# df['CITY'] = df['CITY'].replace('', np.nan)\n",
    "# df['COUNTRY'] = df['COUNTRY'].replace('',np.nan)\n",
    "# df['REAL_DATE'] = df['REAL_DATE'].replace('',np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a36969a2-0bdc-4fec-b0f1-d5ab63631d67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # TITLE과 SUMMARY 열 합치기\n",
    "# df['CONTENTS'] = df['TITLE'] + '\\n' + df['SUMMARY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e29727e8-e499-45df-b079-be9c0321d1f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df=pd.read_csv(\"240418_content_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ebe618b-6207-4804-a998-e96b991af3c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data2 = {'SUMMARY': total_summary_list, 'COUNTRY': total_country_list, 'CITY': total_city_list}\n",
    "df2 = pd.DataFrame(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea368842-ae2c-4645-93f3-dfd531140afe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df0= pd.concat([df,df2], axis=1)\n",
    "df0.to_csv(f\"{temp_dir}/{today}_news_result.csv\", encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee50af04-562a-4677-8112-2f96990ae4bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df0.to_excel(f\"{temp_dir}/{today}_news_result.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442cbe6d-1e71-4ed2-9be2-0993338d5a46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdd5bf31-5764-4ed7-8bb4-9410511c4c0d",
   "metadata": {},
   "source": [
    "### S3에 업로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8be6c0a8-e9a1-4dcc-889b-5d5d03fe385d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "디렉토리 2024-04-30/ 생성 완료\n",
      "파일 2024-04-30_origin.xlsx 업로드 완료\n",
      "파일 2024-04-30_news_result.xlsx 업로드 완료\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "from datetime import date\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# 버킷 이름\n",
    "bucket_name = 'jesamkim-temp-20240409'\n",
    "\n",
    "# 오늘 날짜 가져오기 (YYYY-MM-DD)\n",
    "today = date.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# 오늘 날짜 디렉토리 경로\n",
    "prefix = today + '/'\n",
    "\n",
    "# S3 버킷에 디렉토리 생성\n",
    "try:\n",
    "    s3.put_object(Bucket=bucket_name, Key=prefix)\n",
    "    print(f\"디렉토리 {prefix} 생성 완료\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# 로컬 ./crawlong/yyyy-mm-dd 폴더의 모든 xlsx 파일 업로드\n",
    "for filename in os.listdir(f'{temp_dir}/'):\n",
    "    if filename.endswith('.xlsx'):\n",
    "        local_path = os.path.join(f'{temp_dir}/', filename)\n",
    "        s3_path = os.path.join(prefix, filename)\n",
    "        try:\n",
    "            s3.upload_file(local_path, bucket_name, s3_path)\n",
    "            print(f\"파일 {filename} 업로드 완료\")\n",
    "        except Exception as e:\n",
    "            print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a812b90-a610-4ab0-9073-7636c242a18d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
