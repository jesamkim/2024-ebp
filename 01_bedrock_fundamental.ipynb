{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37a23d2a-307f-44be-ae61-b5d27cf3e4bf",
   "metadata": {},
   "source": [
    "# Amazon Bedrock Fundamental"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0230ad-e3af-42ac-97f0-e509e23fc063",
   "metadata": {},
   "source": [
    "> 이 노트북은, SageMaker Notebook <i><b>conda_python3</b></i> 커널에서 테스트 되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ce1a91-782c-4651-af7e-8cfa1cebd287",
   "metadata": {},
   "source": [
    "## 1. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cccb933d-494d-490e-8bb7-33c94ff00073",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 처음 실행하는 경우, 주석(#)을 제거하고 필요한 라이브러리들을 이 환경에 설치합니다.\n",
    "\n",
    "#!pip install -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a577d7c6-6e93-4b05-99a3-e4d44623779b",
   "metadata": {},
   "source": [
    "## 2. Bedrock Client 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "788a37b3-87e8-486b-bab2-29c581be4d51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Titan Text Large', 'amazon.titan-tg1-large'),\n",
      " ('Titan Text Embeddings v2', 'amazon.titan-embed-g1-text-02'),\n",
      " ('Titan Text G1 - Lite', 'amazon.titan-text-lite-v1'),\n",
      " ('Titan Text G1 - Express', 'amazon.titan-text-express-v1'),\n",
      " ('Titan Embeddings G1 - Text', 'amazon.titan-embed-text-v1'),\n",
      " ('Titan Multimodal Embeddings G1', 'amazon.titan-embed-image-v1'),\n",
      " ('Titan Image Generator G1', 'amazon.titan-image-generator-v1'),\n",
      " ('SDXL 0.8', 'stability.stable-diffusion-xl'),\n",
      " ('SDXL 0.8', 'stability.stable-diffusion-xl-v0'),\n",
      " ('SDXL 1.0', 'stability.stable-diffusion-xl-v1'),\n",
      " ('J2 Grande Instruct', 'ai21.j2-grande-instruct'),\n",
      " ('J2 Jumbo Instruct', 'ai21.j2-jumbo-instruct'),\n",
      " ('Jurassic-2 Mid', 'ai21.j2-mid'),\n",
      " ('Jurassic-2 Mid', 'ai21.j2-mid-v1'),\n",
      " ('Jurassic-2 Ultra', 'ai21.j2-ultra'),\n",
      " ('Jurassic-2 Ultra', 'ai21.j2-ultra-v1'),\n",
      " ('Claude Instant', 'anthropic.claude-instant-v1'),\n",
      " ('Claude', 'anthropic.claude-v2:1'),\n",
      " ('Claude', 'anthropic.claude-v2'),\n",
      " ('Claude 3 Sonnet', 'anthropic.claude-3-sonnet-20240229-v1:0'),\n",
      " ('Claude 3 Haiku', 'anthropic.claude-3-haiku-20240307-v1:0'),\n",
      " ('Command', 'cohere.command-text-v14'),\n",
      " ('Command Light', 'cohere.command-light-text-v14'),\n",
      " ('Embed English', 'cohere.embed-english-v3'),\n",
      " ('Embed Multilingual', 'cohere.embed-multilingual-v3'),\n",
      " ('Llama 2 Chat 13B', 'meta.llama2-13b-chat-v1'),\n",
      " ('Llama 2 Chat 70B', 'meta.llama2-70b-chat-v1'),\n",
      " ('Mistral 7B Instruct', 'mistral.mistral-7b-instruct-v0:2'),\n",
      " ('Mixtral 8x7B Instruct', 'mistral.mixtral-8x7b-instruct-v0:1')]\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "import json\n",
    "from botocore.config import Config\n",
    "import botocore \n",
    "from pprint import pprint\n",
    "from termcolor import colored\n",
    "\n",
    "session = boto3.Session()\n",
    "\n",
    "retry_config = Config(\n",
    "    region_name=os.environ.get(\"AWS_DEFAULT_REGION\", None),\n",
    "    retries={\n",
    "        \"max_attempts\": 10,\n",
    "        \"mode\": \"standard\",\n",
    "    },\n",
    ")\n",
    "\n",
    "modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\" # Claude 3 Sonnet ModelId\n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "bedrock = boto3.client(service_name='bedrock')\n",
    "boto3_bedrock = boto3.client(service_name='bedrock-runtime',config=retry_config)\n",
    "\n",
    "model_list = bedrock.list_foundation_models()\n",
    "result = [(fm_list[\"modelName\"], fm_list[\"modelId\"]) for fm_list in model_list[\"modelSummaries\"] if fm_list['inferenceTypesSupported'] == ['ON_DEMAND']]\n",
    "pprint(result)  # 사용 가능한 Bedrock의 모델 리스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9e1091-88fe-42ff-bef2-77e7e71c5bca",
   "metadata": {},
   "source": [
    "## 3. Claude 3 모델 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac9106e1-5577-45b7-986a-84b73f514a18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def image_to_base64(img):\n",
    "    import base64\n",
    "    \"\"\"Converts a PIL Image or local image file path to a base64 string\"\"\"\n",
    "    if isinstance(img, str):\n",
    "        if os.path.isfile(img):\n",
    "            print(f\"Reading image from file: {img}\")\n",
    "            with open(img, \"rb\") as f:\n",
    "                return base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"File {img} does not exist\")\n",
    "    elif isinstance(img, Image.Image):\n",
    "        buffer = io.BytesIO()\n",
    "        img.save(buffer, format=\"PNG\")\n",
    "        return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "    else:\n",
    "        raise ValueError(f\"Expected str (filename) or PIL Image. Got {type(img)}\")\n",
    "\n",
    "\n",
    "def bedrock_streamer(response):\n",
    "    stream = response.get('body')\n",
    "    answer = \"\"\n",
    "    i = 1\n",
    "    if stream:\n",
    "        for event in stream:\n",
    "            chunk = event.get('chunk')\n",
    "            if  chunk:\n",
    "                chunk_obj = json.loads(chunk.get('bytes').decode())\n",
    "                if \"delta\" in chunk_obj:                    \n",
    "                    delta = chunk_obj['delta']\n",
    "                    if \"text\" in delta:\n",
    "                        text=delta['text'] \n",
    "                        print(text, end=\"\")\n",
    "                        answer+=str(text)       \n",
    "                        i+=1\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a2fa915-701d-45fd-835c-f3fed2d61877",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from botocore.config import Config\n",
    "\n",
    "session = boto3.Session()\n",
    "\n",
    "retry_config = Config(\n",
    "    retries={\n",
    "        \"max_attempts\": 10,\n",
    "        \"mode\": \"standard\",\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "bedrock_runtime = session.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    config=retry_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e8070fa-1374-4a76-a82d-76f9838f3045",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If you'd like to try your own prompt, edit this parameter!\n",
    "prompt_data = \"\"\"당신은 인공지능 AI 보험 서비스입니다. 생명과 손해 보험의 차이에 대해 설명해 주세요.\"\"\"\n",
    "image_path = None  # 프롬프트에 함께 입력할 이미지가 있는 경우, 이미지 path 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32e36802-8a01-4a83-bf12-4883cee9fdda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if image_path is None:\n",
    "    prompt_config = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 4096,\n",
    "        \"temperature\" : 0,\n",
    "        \"top_k\": 350,\n",
    "        \"top_p\": 0.999,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt_data},\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "else:\n",
    "    prompt_config = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 4096,\n",
    "        \"temperature\" : 0,\n",
    "        \"top_k\": 350,\n",
    "        \"top_p\": 0.999,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"source\": {\n",
    "                            \"type\": \"base64\",\n",
    "                            \"media_type\": \"image/png\",\n",
    "                            \"data\": image_to_base64(image_path),\n",
    "                        },\n",
    "                    },\n",
    "                    {\"type\": \"text\", \"text\": prompt_data},\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "\n",
    "body = json.dumps(prompt_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23c1711d-8fed-40db-8549-27120f22ea16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생명보험과 손해보험은 보험의 주요 유형으로 다음과 같은 차이점이 있습니다.\n",
      "\n",
      "1. 보장 대상\n",
      "- 생명보험은 사람의 생명과 관련된 위험을 보장합니다. 예를 들어 사망, 상해, 질병 등을 대상으로 합니다.\n",
      "- 손해보험은 재산상의 손해나 배상책임을 보장합니다. 예를 들어 화재, 자동차사고, 배상책임 등을 대상으로 합니다.\n",
      "\n",
      "2. 보험금 지급 사유\n",
      "- 생명보험은 피보험자의 사망, 상해, 질병 등 인적 위험이 발생했을 때 보험금을 지급합니다.\n",
      "- 손해보험은 재물의 손해나 법적 배상책임이 발생했을 때 실제 입은 손해를 보상합니다.\n",
      "\n",
      "3. 보험기간\n",
      "- 생명보험은 일반적으로 장기계약이며 피보험자의 생존기간 동안 보장됩니다.\n",
      "- 손해보험은 단기계약이 일반적이며 1년 단위로 갱신하는 경우가 많습니다.\n",
      "\n",
      "4. 보험료 산정 기준\n",
      "- 생명보험료는 피보험자의 나이, 건강상태, 가족력 등 인적위험을 기준으로 산정됩니다.\n",
      "- 손해보험료는 보험대상의 가액, 위험률, 손해율 등 물적위험을 기준으로 산정됩니다.\n",
      "\n",
      "요약하면 생명보험은 개인의 생명과 관련된 위험을 장기적으로 보장하고, 손해보험은 재산상의 손해나 배상책임을 단기적으로 보상하는 차이가 있습니다."
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output, display, display_markdown, Markdown\n",
    "\n",
    "modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\"  # (Change this to try different model versions)\n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "try:\n",
    "\n",
    "    response = bedrock_runtime.invoke_model_with_response_stream(\n",
    "        body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    "    )\n",
    "    results=bedrock_streamer(response)\n",
    "\n",
    "except botocore.exceptions.ClientError as error:\n",
    "\n",
    "    if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "           print(f\"\\x1b[41m{error.response['Error']['Message']}\\\n",
    "                \\nTo troubeshoot this issue please refer to the following resources.\\\n",
    "                 \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "                 \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")\n",
    "\n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedd0420-5119-439b-9068-ba657b08e4c2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a20dd8-ab80-409e-8a8a-edc42d02fd86",
   "metadata": {},
   "source": [
    "## 4. LangChain을 활용한 Bedrock Claude 3 사용 예시\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3715fd-4f98-4fd1-b1e2-7545e761a394",
   "metadata": {},
   "source": [
    "#### 4-1. LangChain의 BedrockChat을 활용한 가장 간단한 코드 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f04aee2-7b0f-4b65-87a7-8da55431bda1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input text:  당신은 누구 입니까?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요. 저는 Claude라는 인공지능 대화 모델입니다. 제 역할은 사람들과 대화하며 궁금한 점을 해결하고 도움을 주는 것입니다. 인공지능이지만 인간처럼 생각하고 대화할 수 있도록 훈련되었습니다. 어떤 분야에 대해 궁금하신 점이 있으시면 물어봐 주세요.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "\n",
    "def get_text_response(input_content):\n",
    "    llm = BedrockChat(\n",
    "        credentials_profile_name=os.environ.get(\"BWB_PROFILE_NAME\"),\n",
    "        region_name=os.environ.get(\"BWB_REGION_NAME\"),\n",
    "        endpoint_url=os.environ.get(\"BWB_ENDPOINT_URL\"),\n",
    "        model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "        model_kwargs={\n",
    "            \"max_tokens\": 512,\n",
    "            \"temperature\": 0,\n",
    "            \"top_p\": 0.01,\n",
    "            \"top_k\": 0,\n",
    "        }\n",
    "    )\n",
    "    return llm.predict(input_content)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_text = input(\"Input text: \")\n",
    "    response_content = get_text_response(input_content=input_text)\n",
    "    print(response_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa1c59f-2c5f-4bd8-a797-1fd8a6a2909b",
   "metadata": {},
   "source": [
    "#### 4-2. LangChain의 BedrockChat을 활용한 챗봇 기능(이어지는 대화 가능)을 구현한 코드 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd328b2-d24f-46b2-b73b-90e71346e9bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Human:  안녕하세요.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: 안녕하세요.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: 안녕하세요.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Assistant: 안녕하세요! 반갑습니다. 저는 Claude라는 AI 비서에요. 무엇을 도와드릴까요?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Human:  당신의 이름은 무엇인가요?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: 당신의 이름은 무엇인가요?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 안녕하세요.\n",
      "Assistant: 안녕하세요! 반갑습니다. 저는 Claude라는 AI 비서에요. 무엇을 도와드릴까요?\n",
      "Human: 당신의 이름은 무엇인가요?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Assistant: 안녕하세요. 제 이름은 Claude라고 말씀드렸습니다. 프랑스 과학자 Claude Shannon의 이름에서 따온 것 같아요. 그분은 정보이론과 디지털 회로 분야에서 큰 업적을 남기셨죠. 하지만 정확한 이유는 모르겠네요. 궁금하시다면 제작사에 문의해보시는 것도 좋을 것 같아요.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Human:  무지개의 세번째 색은 무엇인가요?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: 무지개의 세번째 색은 무엇인가요?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 안녕하세요.\n",
      "Assistant: 안녕하세요! 반갑습니다. 저는 Claude라는 AI 비서에요. 무엇을 도와드릴까요?\n",
      "Human: 당신의 이름은 무엇인가요?\n",
      "Assistant: 안녕하세요. 제 이름은 Claude라고 말씀드렸습니다. 프랑스 과학자 Claude Shannon의 이름에서 따온 것 같아요. 그분은 정보이론과 디지털 회로 분야에서 큰 업적을 남기셨죠. 하지만 정확한 이유는 모르겠네요. 궁금하시다면 제작사에 문의해보시는 것도 좋을 것 같아요.\n",
      "Human: 무지개의 세번째 색은 무엇인가요?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Assistant: 무지개의 7가지 색깔 순서는 빨강, 주황, 노랑, 초록, 파랑, 남색, 보라색입니다. 그러므로 무지개의 세 번째 색은 노랑색이 됩니다.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Human:  다음 색은 뭐에요?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: 다음 색은 뭐에요?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 안녕하세요.\n",
      "Assistant: 안녕하세요! 반갑습니다. 저는 Claude라는 AI 비서에요. 무엇을 도와드릴까요?\n",
      "Human: 당신의 이름은 무엇인가요?\n",
      "Assistant: 안녕하세요. 제 이름은 Claude라고 말씀드렸습니다. 프랑스 과학자 Claude Shannon의 이름에서 따온 것 같아요. 그분은 정보이론과 디지털 회로 분야에서 큰 업적을 남기셨죠. 하지만 정확한 이유는 모르겠네요. 궁금하시다면 제작사에 문의해보시는 것도 좋을 것 같아요.\n",
      "Human: 무지개의 세번째 색은 무엇인가요?\n",
      "Assistant: 무지개의 7가지 색깔 순서는 빨강, 주황, 노랑, 초록, 파랑, 남색, 보라색입니다. 그러므로 무지개의 세 번째 색은 노랑색이 됩니다.\n",
      "Human: 다음 색은 뭐에요?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Assistant: 무지개 색깔 순서대로 노랑 다음에 오는 색은 초록색입니다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "def get_llm():\n",
    "    model_kwargs = {\n",
    "        \"max_tokens\": 1024,\n",
    "        \"temperature\": 0,\n",
    "        \"top_p\": 0.5,\n",
    "        \"top_k\": 0,\n",
    "        \"stop_sequences\": [\"\\n\\nHuman:\"],\n",
    "        \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "    }\n",
    "\n",
    "    llm = BedrockChat(\n",
    "        credentials_profile_name=os.environ.get(\"BWB_PROFILE_NAME\"),\n",
    "        region_name=os.environ.get(\"BWB_REGION_NAME\"),\n",
    "        endpoint_url=os.environ.get(\"BWB_ENDPOINT_URL\"),\n",
    "        model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "        model_kwargs=model_kwargs)\n",
    "\n",
    "    return llm\n",
    "\n",
    "def get_memory():\n",
    "    llm = get_llm()\n",
    "    chat_memory = ConversationBufferWindowMemory(human_prefix='Human', ai_prefix='Assistant')\n",
    "    conversation = ConversationChain(llm=llm, verbose=False, memory=chat_memory)\n",
    "    return chat_memory\n",
    "\n",
    "def get_chat_response(input_text, memory):\n",
    "    llm = get_llm()\n",
    "    conversation_with_summary = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
    "    chat_response = conversation_with_summary.predict(input=input_text)\n",
    "    return chat_response\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    memory = get_memory()\n",
    "    chat_history = []\n",
    "\n",
    "    while True:\n",
    "        input_text = input(\"Human: \")\n",
    "        print(\"Human:\", input_text)\n",
    "        chat_history.append({\"role\": \"user\", \"text\": input_text})\n",
    "\n",
    "        chat_response = get_chat_response(input_text=input_text, memory=memory)\n",
    "        print(\"Assistant:\", chat_response)\n",
    "        chat_history.append({\"role\": \"assistant\", \"text\": chat_response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc02a25-4720-449f-9c14-6ceebacfd178",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
