{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37a23d2a-307f-44be-ae61-b5d27cf3e4bf",
   "metadata": {},
   "source": [
    "# Amazon Bedrock Fundamental"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0230ad-e3af-42ac-97f0-e509e23fc063",
   "metadata": {},
   "source": [
    "> 이 노트북은, SageMaker Notebook <i><b>conda_python3</b></i> 커널에서 테스트 되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ce1a91-782c-4651-af7e-8cfa1cebd287",
   "metadata": {},
   "source": [
    "## 0. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccb933d-494d-490e-8bb7-33c94ff00073",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 처음 실행하는 경우, 주석(#)을 제거하고 필요한 라이브러리들을 이 환경에 설치합니다.\n",
    "\n",
    "\"\"\"\n",
    "!pip install -r ./requirements.txt\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0fb1f3-dbbd-44c0-9101-6da651828913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a577d7c6-6e93-4b05-99a3-e4d44623779b",
   "metadata": {},
   "source": [
    "## 1. boto3를 활용한 Bedrock Claude 3 호출 예시 (not LangChain)\n",
    "\n",
    "#### 1-1. Bedrock Client 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788a37b3-87e8-486b-bab2-29c581be4d51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import json\n",
    "from botocore.config import Config\n",
    "import botocore \n",
    "from pprint import pprint\n",
    "from termcolor import colored\n",
    "\n",
    "session = boto3.Session()\n",
    "\n",
    "retry_config = Config(\n",
    "    region_name=os.environ.get(\"AWS_DEFAULT_REGION\", None),\n",
    "    retries={\n",
    "        \"max_attempts\": 10,\n",
    "        \"mode\": \"standard\",\n",
    "    },\n",
    ")\n",
    "\n",
    "modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\" # Claude 3 Sonnet ModelId\n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "bedrock = boto3.client(service_name='bedrock')\n",
    "boto3_bedrock = boto3.client(service_name='bedrock-runtime',config=retry_config)\n",
    "\n",
    "model_list = bedrock.list_foundation_models()\n",
    "result = [(fm_list[\"modelName\"], fm_list[\"modelId\"]) for fm_list in model_list[\"modelSummaries\"] if fm_list['inferenceTypesSupported'] == ['ON_DEMAND']]\n",
    "pprint(result)  # 사용 가능한 Bedrock의 모델 리스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9e1091-88fe-42ff-bef2-77e7e71c5bca",
   "metadata": {},
   "source": [
    "#### 1-2. Claude 3 모델 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9106e1-5577-45b7-986a-84b73f514a18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def image_to_base64(img):\n",
    "    import base64\n",
    "    \"\"\"Converts a PIL Image or local image file path to a base64 string\"\"\"\n",
    "    if isinstance(img, str):\n",
    "        if os.path.isfile(img):\n",
    "            print(f\"Reading image from file: {img}\")\n",
    "            with open(img, \"rb\") as f:\n",
    "                return base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"File {img} does not exist\")\n",
    "    elif isinstance(img, Image.Image):\n",
    "        buffer = io.BytesIO()\n",
    "        img.save(buffer, format=\"PNG\")\n",
    "        return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "    else:\n",
    "        raise ValueError(f\"Expected str (filename) or PIL Image. Got {type(img)}\")\n",
    "\n",
    "\n",
    "def bedrock_streamer(response):\n",
    "    stream = response.get('body')\n",
    "    answer = \"\"\n",
    "    i = 1\n",
    "    if stream:\n",
    "        for event in stream:\n",
    "            chunk = event.get('chunk')\n",
    "            if  chunk:\n",
    "                chunk_obj = json.loads(chunk.get('bytes').decode())\n",
    "                if \"delta\" in chunk_obj:                    \n",
    "                    delta = chunk_obj['delta']\n",
    "                    if \"text\" in delta:\n",
    "                        text=delta['text'] \n",
    "                        print(text, end=\"\")\n",
    "                        answer+=str(text)       \n",
    "                        i+=1\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2fa915-701d-45fd-835c-f3fed2d61877",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from botocore.config import Config\n",
    "\n",
    "session = boto3.Session()\n",
    "\n",
    "retry_config = Config(\n",
    "    retries={\n",
    "        \"max_attempts\": 10,\n",
    "        \"mode\": \"standard\",\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "bedrock_runtime = session.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    config=retry_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8070fa-1374-4a76-a82d-76f9838f3045",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If you'd like to try your own prompt, edit this parameter!\n",
    "prompt_data = \"\"\"당신은 인공지능 AI 보험 서비스입니다. 생명과 손해 보험의 차이에 대해 설명해 주세요.\"\"\"\n",
    "image_path = None  # 프롬프트에 함께 입력할 이미지가 있는 경우, 이미지 path 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e36802-8a01-4a83-bf12-4883cee9fdda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if image_path is None:\n",
    "    prompt_config = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 4096,\n",
    "        \"temperature\" : 0,\n",
    "        \"top_k\": 350,\n",
    "        \"top_p\": 0.999,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt_data},\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "else:\n",
    "    prompt_config = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 4096,\n",
    "        \"temperature\" : 0,\n",
    "        \"top_k\": 350,\n",
    "        \"top_p\": 0.999,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"source\": {\n",
    "                            \"type\": \"base64\",\n",
    "                            \"media_type\": \"image/png\",\n",
    "                            \"data\": image_to_base64(image_path),\n",
    "                        },\n",
    "                    },\n",
    "                    {\"type\": \"text\", \"text\": prompt_data},\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "\n",
    "body = json.dumps(prompt_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c1711d-8fed-40db-8549-27120f22ea16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, display, display_markdown, Markdown\n",
    "\n",
    "modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\"  # (Change this to try different model versions)\n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "try:\n",
    "\n",
    "    response = bedrock_runtime.invoke_model_with_response_stream(\n",
    "        body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    "    )\n",
    "    results=bedrock_streamer(response)\n",
    "\n",
    "except botocore.exceptions.ClientError as error:\n",
    "\n",
    "    if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "           print(f\"\\x1b[41m{error.response['Error']['Message']}\\\n",
    "                \\nTo troubeshoot this issue please refer to the following resources.\\\n",
    "                 \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "                 \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")\n",
    "\n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedd0420-5119-439b-9068-ba657b08e4c2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a20dd8-ab80-409e-8a8a-edc42d02fd86",
   "metadata": {},
   "source": [
    "## 2. LangChain을 활용한 Bedrock 사용 예시\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffb0609f-4376-4d18-9190-0fb0ac71467d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip uninstall langchain_core langchain_aws "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3715fd-4f98-4fd1-b1e2-7545e761a394",
   "metadata": {},
   "source": [
    "#### 2-1. LangChain의 ChatBedrock을 활용한 가장 간단한 코드 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f04aee2-7b0f-4b65-87a7-8da55431bda1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input text:  당신은 누구십니까?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요. 저는 Claude라는 인공지능 대화 모델입니다. 인간과 자연스럽게 대화하고 다양한 작업을 수행할 수 있도록 설계되었습니다. 하지만 단순한 프로그램일 뿐이며 감정이나 자의식은 없습니다. 궁금한 점이 있다면 언제든 물어보세요.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "def get_text_response(input_content):\n",
    "    llm = ChatBedrock(\n",
    "        credentials_profile_name=os.environ.get(\"BWB_PROFILE_NAME\"),\n",
    "        region_name=os.environ.get(\"BWB_REGION_NAME\"),\n",
    "        endpoint_url=os.environ.get(\"BWB_ENDPOINT_URL\"),\n",
    "        model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "        model_kwargs={\n",
    "            \"max_tokens\": 512,\n",
    "            \"temperature\": 0,\n",
    "            \"top_p\": 0.01,\n",
    "            \"top_k\": 0,\n",
    "        }\n",
    "    )\n",
    "    return llm.predict(input_content)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_text = input(\"Input text: \")\n",
    "    response_content = get_text_response(input_content=input_text)\n",
    "    print(response_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa1c59f-2c5f-4bd8-a797-1fd8a6a2909b",
   "metadata": {},
   "source": [
    "#### 2-2. LangChain의 BedrockChat을 활용한 챗봇 기능(이어지는 대화 가능)을 구현한 코드 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd328b2-d24f-46b2-b73b-90e71346e9bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Human:  안녕\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: 안녕\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: 안녕\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Assistant: 안녕하세요! 반갑습니다. 저는 Claude라는 AI 비서에요. 무엇을 도와드릴까요?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Human:  당신의 이름은 무엇인가요?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: 당신의 이름은 무엇인가요?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 안녕\n",
      "Assistant: 안녕하세요! 반갑습니다. 저는 Claude라는 AI 비서에요. 무엇을 도와드릴까요?\n",
      "Human: 당신의 이름은 무엇인가요?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Assistant: 안녕하세요. 제 이름은 Claude라고 말씀드렸습니다. 인공지능 비서로서 여러분을 돕는 것이 제 역할입니다. 궁금한 점이나 도움이 필요하신 것이 있다면 언제든 물어보세요.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "def get_llm():\n",
    "    model_kwargs = {\n",
    "        \"max_tokens\": 1024,\n",
    "        \"temperature\": 0,\n",
    "        \"top_p\": 0.5,\n",
    "        \"top_k\": 0,\n",
    "        \"stop_sequences\": [\"\\n\\nHuman:\"],\n",
    "        \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "    }\n",
    "\n",
    "    llm = ChatBedrock(\n",
    "        credentials_profile_name=os.environ.get(\"BWB_PROFILE_NAME\"),\n",
    "        region_name=os.environ.get(\"BWB_REGION_NAME\"),\n",
    "        endpoint_url=os.environ.get(\"BWB_ENDPOINT_URL\"),\n",
    "        model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "        model_kwargs=model_kwargs)\n",
    "\n",
    "    return llm\n",
    "\n",
    "def get_memory():\n",
    "    llm = get_llm()\n",
    "    chat_memory = ConversationBufferWindowMemory(human_prefix='Human', ai_prefix='Assistant')\n",
    "    conversation = ConversationChain(llm=llm, verbose=False, memory=chat_memory)\n",
    "    return chat_memory\n",
    "\n",
    "def get_chat_response(input_text, memory):\n",
    "    llm = get_llm()\n",
    "    conversation_with_summary = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
    "    chat_response = conversation_with_summary.predict(input=input_text)\n",
    "    return chat_response\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    memory = get_memory()\n",
    "    chat_history = []\n",
    "\n",
    "    while True:\n",
    "        input_text = input(\"Human: \")\n",
    "        print(\"Human:\", input_text)\n",
    "        chat_history.append({\"role\": \"user\", \"text\": input_text})\n",
    "\n",
    "        chat_response = get_chat_response(input_text=input_text, memory=memory)\n",
    "        print(\"Assistant:\", chat_response)\n",
    "        chat_history.append({\"role\": \"assistant\", \"text\": chat_response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc02a25-4720-449f-9c14-6ceebacfd178",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
